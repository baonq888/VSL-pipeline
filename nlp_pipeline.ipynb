{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "154cc65d",
      "metadata": {
        "id": "154cc65d"
      },
      "source": [
        "# Vietnamese Text Processing Pipeline\n",
        "\n",
        "This notebook implement NLP pipeline for Vietnamese including:\n",
        "1. Tokenization\n",
        "2. POS Tagging\n",
        "3. Tone Processing (diacritic stripping)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "02SEUmPxEiG3"
      },
      "id": "02SEUmPxEiG3"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8c4721e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c4721e6",
        "outputId": "301dc816-7c94-4b9a-97be-eeb6d8d60391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting py_vncorenlp\n",
            "  Downloading py_vncorenlp-0.1.4.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from underthesea) (8.2.1)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.5.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.2)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp311-cp311-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pyjnius (from py_vncorenlp)\n",
            "  Downloading pyjnius-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->underthesea) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (3.6.0)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading underthesea_core-1.0.4-cp311-cp311-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyjnius-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: py_vncorenlp\n",
            "  Building wheel for py_vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py_vncorenlp: filename=py_vncorenlp-0.1.4-py3-none-any.whl size=4304 sha256=04ac090f07ed1ad9f3a745edcee5721a0f1a8d1c02a4914fcc918cdc112a23ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/2d/d6/158260bfd6820d144535857b80cc112bee5c3aa6d81b6dc049\n",
            "Successfully built py_vncorenlp\n",
            "Installing collected packages: underthesea-core, pyjnius, python-crfsuite, py_vncorenlp, underthesea, sklearn-crfsuite, pyvi\n",
            "Successfully installed py_vncorenlp-0.1.4 pyjnius-1.6.1 python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0 underthesea-6.8.4 underthesea-core-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip3 install underthesea pyvi py_vncorenlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction to VnCoreNLP\n",
        "\n",
        "\n",
        "**VnCoreNLP** is a lightweight and fast Vietnamese NLP toolkit developed to provide robust and accurate core language processing modules, especially designed for low-resource languages like Vietnamese. It integrates multiple NLP tasks into one pipeline without requiring GPU or neural network frameworks.\n",
        "\n",
        "The toolkit is described in the paper:\n",
        "\n",
        "> Nguyen, Dat Quoc, Thanh Vu, and Anh Tuan Nguyen.  \n",
        "> **\"VnCoreNLP: A Vietnamese Natural Language Processing Toolkit.\"**  \n",
        "> *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations*. 2018.  \n",
        "> [https://aclanthology.org/N18-5012](https://aclanthology.org/N18-5012)\n",
        "\n",
        "## Key Components\n",
        "\n",
        "VnCoreNLP provides four main modules:\n",
        "\n",
        "### 1. `wseg` – Word Segmentation\n",
        "Vietnamese uses white spaces to separate syllables, not words. VnCoreNLP uses a **transformation rule-based learning model** to segment words accurately and efficiently.\n",
        "\n",
        "### 2. `pos` – Part-of-Speech Tagging\n",
        "Uses the **MarMoT** CRF-based sequence tagger. It achieves **95.88%** accuracy and outperforms deep neural networks in both speed and reliability.\n",
        "\n",
        "### 3. `ner` – Named Entity Recognition\n",
        "Applies a **dynamic feature induction model** that learns the best combinations of features to identify entities like persons, locations, and organizations.\n",
        "\n",
        "### 4. `parse` – Dependency Parsing\n",
        "Implements a **greedy transition-based dependency parser** for building syntactic dependency trees with high speed and competitive accuracy.\n"
      ],
      "metadata": {
        "id": "SVYh_jcUFsS5"
      },
      "id": "SVYh_jcUFsS5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model"
      ],
      "metadata": {
        "id": "64PUrU6Zb734"
      },
      "id": "64PUrU6Zb734"
    },
    {
      "cell_type": "markdown",
      "id": "cf6bcb1c",
      "metadata": {
        "id": "cf6bcb1c"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "92a18d27",
      "metadata": {
        "id": "92a18d27"
      },
      "outputs": [],
      "source": [
        "from underthesea import word_tokenize as uts_tokenize, pos_tag as uts_pos_tag\n",
        "import unicodedata\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e460ad8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e460ad8c",
        "outputId": "67943682-f3eb-49d3-ba5a-b14a7c724e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "{0: [{'index': 1, 'wordForm': 'Ông', 'posTag': 'Nc', 'nerLabel': 'O', 'head': 4, 'depLabel': 'sub'}, {'index': 2, 'wordForm': 'Nguyễn_Khắc_Chúc', 'posTag': 'Np', 'nerLabel': 'B-PER', 'head': 1, 'depLabel': 'nmod'}, {'index': 3, 'wordForm': 'đang', 'posTag': 'R', 'nerLabel': 'O', 'head': 4, 'depLabel': 'adv'}, {'index': 4, 'wordForm': 'làm_việc', 'posTag': 'V', 'nerLabel': 'O', 'head': 0, 'depLabel': 'root'}, {'index': 5, 'wordForm': 'tại', 'posTag': 'E', 'nerLabel': 'O', 'head': 4, 'depLabel': 'loc'}, {'index': 6, 'wordForm': 'Đại_học', 'posTag': 'N', 'nerLabel': 'B-ORG', 'head': 5, 'depLabel': 'pob'}, {'index': 7, 'wordForm': 'Quốc_gia', 'posTag': 'N', 'nerLabel': 'I-ORG', 'head': 6, 'depLabel': 'nmod'}, {'index': 8, 'wordForm': 'Hà_Nội', 'posTag': 'Np', 'nerLabel': 'I-ORG', 'head': 6, 'depLabel': 'nmod'}, {'index': 9, 'wordForm': '.', 'posTag': 'CH', 'nerLabel': 'O', 'head': 4, 'depLabel': 'punct'}]}\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "import py_vncorenlp\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set model path\n",
        "save_dir = \"/content/drive/MyDrive/vncorenlp\"\n",
        "\n",
        "# Delete the entire folder if it exists\n",
        "if os.path.exists(save_dir):\n",
        "    shutil.rmtree(save_dir)\n",
        "\n",
        "# Recreate empty directory\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Download the VnCoreNLP model\n",
        "py_vncorenlp.download_model(save_dir=save_dir)\n",
        "\n",
        "# Load the model\n",
        "model = py_vncorenlp.VnCoreNLP(save_dir=save_dir)\n",
        "\n",
        "# Test the model\n",
        "text = \"Ông Nguyễn Khắc Chúc đang làm việc tại Đại học Quốc gia Hà Nội.\"\n",
        "print(model.annotate_text(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Text Preprocessing"
      ],
      "metadata": {
        "id": "ckRpoMApYEeI"
      },
      "id": "ckRpoMApYEeI"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from underthesea import text_normalize"
      ],
      "metadata": {
        "id": "LV6vrE7FYuST"
      },
      "id": "LV6vrE7FYuST",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html(sentence):\n",
        "  return re.sub(r'<[^>]*>', '', str(sentence))\n",
        "\n",
        "def remove_tags(sentence):\n",
        "  \"\"\"\n",
        "    Chuẩn hoá văn bản: xoá tag\n",
        "  \"\"\"\n",
        "  return re.sub(r'@\\w*', '', sentence).strip()\n",
        "\n",
        "def remove_emoji(sentence):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE\n",
        "    )\n",
        "    return emoji_pattern.sub(r'', sentence)\n",
        "\n",
        "def remove_punctuation(sentence):\n",
        "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
        "    no_punctuation = sentence.translate(translator)\n",
        "    return ' '.join(no_punctuation.split())\n",
        "\n",
        "def to_lower(sentence):\n",
        "    return sentence.lower()\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Chuẩn hoá văn bản: xoá khoảng trắng dư, chuẩn Unicode NFC\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return unicodedata.normalize('NFC', text)\n",
        "\n",
        "def normalize_bar(sentence):\n",
        "  sentence = text_normalize(sentence)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "9gAB4lNSciT8"
      },
      "id": "9gAB4lNSciT8",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization/POS tagging/NER/Dependency Parsing"
      ],
      "metadata": {
        "id": "6tqTkIB1bfWr"
      },
      "id": "6tqTkIB1bfWr"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "bda40ff4",
      "metadata": {
        "id": "bda40ff4"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import json\n",
        "\n",
        "POS_TAG_MEANING = {\n",
        "    \"Np\": \"Proper noun\",\n",
        "    \"N\": \"Common noun\",\n",
        "    \"V\": \"Verb\",\n",
        "    \"R\": \"Adverb\",\n",
        "    \"E\": \"Preposition\",\n",
        "    \"CH\": \"Punctuation\",\n",
        "}\n",
        "\n",
        "\n",
        "def vn_pipeline(text, model):\n",
        "    text = remove_html(text)\n",
        "    text = remove_tags(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = to_lower(text)\n",
        "    text = normalize_text(text)\n",
        "\n",
        "    results = model.annotate_text(text)\n",
        "\n",
        "    # Convert raw result to JSON string\n",
        "    raw_json = json.dumps(results, ensure_ascii=False, indent=2)\n",
        "\n",
        "    processed = []\n",
        "    for sentence in results.values():\n",
        "        for token in sentence:\n",
        "            word = token['wordForm']\n",
        "            pos = token['posTag']\n",
        "            head = token['head']\n",
        "            dep = token['depLabel']\n",
        "            index = token['index']\n",
        "            meaning = POS_TAG_MEANING.get(pos, \"Unknown\")\n",
        "            processed.append({\n",
        "                \"index\": index,\n",
        "                \"wordForm\": word,\n",
        "                \"pos\": pos,\n",
        "                \"pos_meaning\": meaning,\n",
        "                \"head\": head,\n",
        "                \"dep\": dep\n",
        "            })\n",
        "    return processed, raw_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "oTKUKe1CCD_M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTKUKe1CCD_M",
        "outputId": "15140859-1477-4b12-c0f2-ec878b478f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Raw JSON Output:\n",
            " {\n",
            "  \"0\": [\n",
            "    {\n",
            "      \"index\": 1,\n",
            "      \"wordForm\": \"nguyễn_khắc\",\n",
            "      \"posTag\": \"N\",\n",
            "      \"nerLabel\": \"O\",\n",
            "      \"head\": 4,\n",
            "      \"depLabel\": \"sub\"\n",
            "    },\n",
            "    {\n",
            "      \"index\": 2,\n",
            "      \"wordForm\": \"chúc\",\n",
            "      \"posTag\": \"V\",\n",
            "      \"nerLabel\": \"O\",\n",
            "      \"head\": 1,\n",
            "      \"depLabel\": \"nmod\"\n",
            "    },\n",
            "    {\n",
            "      \"index\": 3,\n",
            "      \"wordForm\": \"đang\",\n",
            "      \"posTag\": \"R\",\n",
            "      \"nerLabel\": \"O\",\n",
            "      \"head\": 4,\n",
            "      \"depLabel\": \"adv\"\n",
            "    },\n",
            "    {\n",
            "      \"index\": 4,\n",
            "      \"wordForm\": \"làm_việc\",\n",
            "      \"posTag\": \"V\",\n",
            "      \"nerLabel\": \"O\",\n",
            "      \"head\": 0,\n",
            "      \"depLabel\": \"root\"\n",
            "    },\n",
            "    {\n",
            "      \"index\": 5,\n",
            "      \"wordForm\": \"tại\",\n",
            "      \"posTag\": \"E\",\n",
            "      \"nerLabel\": \"O\",\n",
            "      \"head\": 4,\n",
            "      \"depLabel\": \"loc\"\n",
            "    },\n",
            "    {\n",
            "      \"index\": 6,\n",
            "      \"wordForm\": \"đại_học\",\n",
            "      \"posTag\": \"N\",\n",
            "      \"nerLabel\": \"B-ORG\",\n",
            "      \"head\": 5,\n",
            "      \"depLabel\": \"pob\"\n",
            "    },\n",
            "    {\n",
            "      \"index\": 7,\n",
            "      \"wordForm\": \"quốc_gia\",\n",
            "      \"posTag\": \"N\",\n",
            "      \"nerLabel\": \"I-ORG\",\n",
            "      \"head\": 6,\n",
            "      \"depLabel\": \"nmod\"\n",
            "    },\n",
            "    {\n",
            "      \"index\": 8,\n",
            "      \"wordForm\": \"hà_nội\",\n",
            "      \"posTag\": \"N\",\n",
            "      \"nerLabel\": \"I-ORG\",\n",
            "      \"head\": 4,\n",
            "      \"depLabel\": \"loc\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "text = \"Nguyễn Khắc Chúc đang làm việc tại Đại học Quốc gia Hà Nội.\"\n",
        "result, raw_json = vn_pipeline(text, model)\n",
        "\n",
        "\n",
        "# Print the raw JSON if needed\n",
        "print(\"\\nRaw JSON Output:\\n\", raw_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vietnamese to VSL Syntax Transformation\n",
        "\n",
        "This notebook implements a **rule-based system** to translate written Vietnamese sentences into the grammatical structure of **Vietnamese Sign Language (VSL)**. The approach is based on the syntax transformation algorithm proposed in the following research paper:\n",
        "\n",
        "> **Thi Bich Diep Nguyen, Trung-Nghia Phung**,  \n",
        "> *Some issues on syntax transformation in Vietnamese sign language translation*  \n",
        "> *International Journal of Computer Science and Network Security, VOL.17 No.5, pp. 292-298, May 2017.*  \n",
        "> [Link to paper](http://paper.ijcsns.org/07_book/201705/20170540.pdf)\n",
        "\n",
        "---\n",
        "\n",
        "## Core Concept\n",
        "\n",
        "The core of the method is to **transform the word order** of a Vietnamese sentence to match the common **Subject-Object-Verb (SOV)** or other patterns used in VSL. This is achieved through:\n",
        "\n",
        "### 1. Parsing the Sentence\n",
        "The input Vietnamese sentence is analyzed using a **dependency parser** (`py_vncorenlp`) to identify its grammatical components, such as:\n",
        "- Subject (`sub`)\n",
        "- Verb (`root`)\n",
        "- Object (`obj`)\n",
        "\n",
        "### 2. Applying Transformation Rules\n",
        "Based on the parsed structure, a set of **predefined rules** are applied to reorder these components. The specific rule depends on the **sentence type**. For example:\n",
        "\n",
        "#### • Simple Sentences\n",
        "Transformed from **SVO** to **SOV**:\n",
        "> *\"Mẹ nấu cơm\"* → **\"Mẹ cơm nấu\"**\n",
        "\n",
        "#### • Negative Sentences\n",
        "The **negation word** (e.g., `\"không\"`) is **moved to the end** of the sentence:\n",
        "> *\"Bố không ăn táo\"* → **\"Bố ăn táo không\"**\n",
        "\n",
        "---\n",
        "\n",
        "## Implementation\n",
        "\n",
        "The Python function `convert_to_vsl` in this notebook implements these transformation rules by:\n",
        "- Identifying the sentence components from the `py_vncorenlp` output\n",
        "- Reassembling them in the target **VSL order**"
      ],
      "metadata": {
        "id": "NXtIZXjyD0HH"
      },
      "id": "NXtIZXjyD0HH"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "aEq_mYPXCGBm",
      "metadata": {
        "id": "aEq_mYPXCGBm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def get_full_phrase(head_index, all_words_dict):\n",
        "    phrase_words = []\n",
        "    nodes_to_visit = [head_index]\n",
        "    visited_indices = set()\n",
        "\n",
        "    while nodes_to_visit:\n",
        "        current_index = nodes_to_visit.pop(0)\n",
        "        if current_index in visited_indices:\n",
        "            continue\n",
        "        visited_indices.add(current_index)\n",
        "\n",
        "        phrase_words.append(all_words_dict[current_index])\n",
        "\n",
        "        children = [w for w in all_words_dict.values() if w.get('head') == current_index]\n",
        "        nodes_to_visit.extend([c['index'] for c in children])\n",
        "\n",
        "    phrase_words.sort(key=lambda w: w['index'])\n",
        "\n",
        "    return \" \".join(w['wordForm'] for w in phrase_words)\n",
        "\n",
        "\n",
        "def convert_to_vsl(sentence_annotation):\n",
        "\n",
        "    # Filter out punctuation and ensure each item is a dictionary\n",
        "    words = [w for w in sentence_annotation if w.get('dep') != 'punct']\n",
        "    if not words:\n",
        "        return \"\"\n",
        "\n",
        "    words_dict = {w['index']: w for w in words}\n",
        "    root = None\n",
        "    subject_head = None\n",
        "    object_head = None\n",
        "    negation_word = None\n",
        "\n",
        "    for word in words:\n",
        "        if word.get('dep') == 'root':\n",
        "            root = word\n",
        "        elif word.get('dep') == 'sub':\n",
        "            subject_head = word\n",
        "        elif word.get('dep') == 'obj':\n",
        "            object_head = word\n",
        "\n",
        "    if not root:\n",
        "        return \" \".join(w['wordForm'] for w in words)\n",
        "\n",
        "    for word in words:\n",
        "        if word.get('wordForm') == 'không' and word.get('head') == root['index']:\n",
        "            negation_word = word\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    subject_phrase = get_full_phrase(subject_head['index'], words_dict) if subject_head else \"\"\n",
        "    verb_phrase = get_full_phrase(root['index'], words_dict) if root else \"\"\n",
        "\n",
        "    object_phrase = get_full_phrase(object_head['index'], words_dict) if object_head else \"\"\n",
        "\n",
        "    final_words = []\n",
        "\n",
        "\n",
        "    if negation_word:\n",
        "        verb_word = root['wordForm']\n",
        "        final_words = [subject_phrase, verb_word, object_phrase, \"không\"]\n",
        "\n",
        "    else:\n",
        "        verb_word = root['wordForm']\n",
        "\n",
        "        other_predicate_phrases = []\n",
        "        predicate_children = [w for w in words if w.get('head') == root['index'] and w.get('dep') not in ['sub', 'obj']]\n",
        "        for child in predicate_children:\n",
        "            other_predicate_phrases.append(get_full_phrase(child['index'], words_dict))\n",
        "\n",
        "        other_predicate_text = \" \".join(other_predicate_phrases)\n",
        "\n",
        "        final_words = [subject_phrase, object_phrase, other_predicate_text, verb_word]\n",
        "\n",
        "    vsl_sentence = \" \".join(filter(None, final_words))\n",
        "    vsl_sentence = vsl_sentence.replace('_', ' ')\n",
        "    return \" \".join(vsl_sentence.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "Uw_DHk_B96Ah",
      "metadata": {
        "id": "Uw_DHk_B96Ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70f58bb-0e40-4485-ea98-5986bbfb51d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Mẹ nấu cơm ngon\n",
            "VSL     : mẹ cơm ngon nấu\n",
            "\n",
            "Original: Công nghệ thông tin đã thay đổi hoàn toàn cách chúng ta giao tiếp và làm việc hàng ngày.\n",
            "VSL     : công nghệ thông tin đã hoàn toàn cách chúng ta giao tiếp và làm việc hàng ngày thay đổi\n",
            "\n",
            "Original: Giáo dục trực tuyến đang trở thành một xu hướng phổ biến, đặc biệt là trong bối cảnh đại dịch.\n",
            "VSL     : giáo dục trực tuyến đang một xu hướng phổ biến đặc biệt là trong bối cảnh đại dịch trở thành\n",
            "\n",
            "Original: Phát triển bền vững là mục tiêu quan trọng mà các quốc gia trên thế giới đều đang hướng tới.\n",
            "VSL     : các quốc gia trên thế giới bền vững là mục tiêu quan trọng mà các quốc gia trên thế giới đều đang hướng tới phát triển\n",
            "\n",
            "Original: Văn học Việt Nam có một lịch sử lâu đời với nhiều tác phẩm kinh điển được lưu truyền qua nhiều thế hệ.\n",
            "VSL     : văn học việt nam một lịch sử lâu đời với nhiều tác phẩm kinh điển được lưu truyền qua nhiều thế hệ có\n",
            "\n",
            "Original: Trí tuệ nhân tạo không chỉ giúp tự động hóa các quy trình mà còn mở ra nhiều cơ hội mới.\n",
            "VSL     : trí tuệ nhân tạo không chỉ tự động hoá các quy trình mà còn mở ra nhiều cơ hội mới giúp\n",
            "\n",
            "Original: Biến đổi khí hậu đang gây ra những ảnh hưởng nghiêm trọng đến môi trường và cuộc sống của con người.\n",
            "VSL     : khí hậu đang gây ra những ảnh hưởng nghiêm trọng đến môi trường và cuộc sống của con người biến đổi\n",
            "\n",
            "Original: Du lịch khám phá những vùng đất mới mang lại cho chúng ta những trải nghiệm văn hóa độc đáo.\n",
            "VSL     : khám phá những vùng đất mới mang lại cho chúng ta những trải nghiệm văn hoá độc đáo du lịch\n",
            "\n",
            "Original: Nông nghiệp thông minh ứng dụng công nghệ cao để tăng năng suất và bảo vệ môi trường.\n",
            "VSL     : nông nghiệp thông minh công nghệ cao để tăng năng suất và bảo vệ môi trường ứng dụng\n",
            "\n",
            "Original: Sức khỏe tinh thần cũng quan trọng không kém gì sức khỏe thể chất trong cuộc sống hiện đại.\n",
            "VSL     : sức khoẻ tinh thần cũng không kém gì sức khoẻ thể chất trong cuộc sống hiện đại quan trọng\n",
            "\n",
            "Original: Âm nhạc dân gian Việt Nam phản ánh đời sống và tâm hồn của người dân qua từng giai điệu.\n",
            "VSL     : âm nhạc dân gian việt nam đời sống và tâm hồn của người dân qua từng giai điệu phản ánh\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# List of 10 Vietnamese sentences for testing\n",
        "vietnamese_sentences = [\n",
        "    \"Mẹ nấu cơm ngon\",\n",
        "    \"Công nghệ thông tin đã thay đổi hoàn toàn cách chúng ta giao tiếp và làm việc hàng ngày.\",\n",
        "    \"Giáo dục trực tuyến đang trở thành một xu hướng phổ biến, đặc biệt là trong bối cảnh đại dịch.\",\n",
        "    \"Phát triển bền vững là mục tiêu quan trọng mà các quốc gia trên thế giới đều đang hướng tới.\",\n",
        "    \"Văn học Việt Nam có một lịch sử lâu đời với nhiều tác phẩm kinh điển được lưu truyền qua nhiều thế hệ.\",\n",
        "    \"Trí tuệ nhân tạo không chỉ giúp tự động hóa các quy trình mà còn mở ra nhiều cơ hội mới.\",\n",
        "    \"Biến đổi khí hậu đang gây ra những ảnh hưởng nghiêm trọng đến môi trường và cuộc sống của con người.\",\n",
        "    \"Du lịch khám phá những vùng đất mới mang lại cho chúng ta những trải nghiệm văn hóa độc đáo.\",\n",
        "    \"Nông nghiệp thông minh ứng dụng công nghệ cao để tăng năng suất và bảo vệ môi trường.\",\n",
        "    \"Sức khỏe tinh thần cũng quan trọng không kém gì sức khỏe thể chất trong cuộc sống hiện đại.\",\n",
        "    \"Âm nhạc dân gian Việt Nam phản ánh đời sống và tâm hồn của người dân qua từng giai điệu.\"\n",
        "]\n",
        "\n",
        "# Process each sentence and print the results\n",
        "for sentence in vietnamese_sentences:\n",
        "    # 1. Annotate the sentence using the vncorenlp model\n",
        "    result, raw_json = vn_pipeline(sentence, model)\n",
        "\n",
        "    # Assuming the first sentence result is the one we want\n",
        "    if len(result) > 0:\n",
        "        sentence_annotation = result\n",
        "\n",
        "        # 2. Convert the annotated sentence to VSL\n",
        "        vsl_sentence = convert_to_vsl(sentence_annotation)\n",
        "\n",
        "        # 3. Print the original and converted sentences\n",
        "        print(f\"Original: {sentence}\")\n",
        "        print(f\"VSL     : {vsl_sentence}\\n\")\n",
        "    else:\n",
        "        print(f\"Could not process sentence: {sentence}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSahBZnla7Ju"
      },
      "id": "xSahBZnla7Ju",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}